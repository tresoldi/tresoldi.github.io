---
layout: post
title: "On Manifolds, Code, and the Infrastructure Between"
date: 2025-10-13 08:00:00 +0200
tags: [row2vec, embeddings, llm agents, infrastructure, craftsmanship, computational linguistics]
---

I've just released [row2vec](https://github.com/evotext/row2vec), a library for generating low-dimensional embeddings from tabular data -- it takes rows of mixed content in a dataset, and transforms each one into a series of numbers which are more easily comparable one with the other. Neural autoencoders, classical methods (PCA, t-SNE, UMAP), neural architecture search, contrastive learning for manuscripts, there is a lot in such a small library. It was originally built for Michael's Cultural Evolution of Texts project, specifically for modeling runestave evolution, but designed as infrastructure for problems I haven't encountered yet.

The library is part of what might look like a sudden burst of productivity: `asymcat`, `freqprob`, `arcaverborum`, now `row2vec` (and more stuff on the line to be released). It does not come from productivity, but from acceptance. These projects have been hanging around, half-finished, over-thought, waiting for me to stop insisting they be perfectly polished before release. And here's the part that might surprise some people: I built a lot of this with the help of LLM coding agents. I'm probably more skeptical about AI-assisted coding than most people actually using it, the way I'm skeptical about Bayesian phylogenetics, but I have been using it anyway. Carefully. Slowly. 

Let me explain what I mean by that, and why it connects to `row2vec` in the high spheres.

## The Interpolation Problem

From a machine learning perspective, the runestaves problem was mostly about high cardinality. Hundreds, sometimes thousands, of distinct categorical values (glyphs, variants, witnesses) where most behave in virtually identical ways. The challenge is discovering which dimensions actually vary, particularly when there are interaction effects. Most data is redundant and we need to find the manifold where the real structure lives, the lower-dimensional space where the meaningful differences are encoded.

This is what `row2vec` does. You feed it a DataFrame with numeric and categorical columns, specify an embedding dimension, and it learns a projection. But which projection? PCA assumes linear structure. t-SNE preserves local neighborhoods. UMAP tries to balance local and global topology. Neural autoencoders can capture nonlinear patterns but need architecture decisions (layers, width, dropout, etc.). The methodological pluralism isn't agnosticism. It's honest uncertainty about which projection preserves what you need for the next problem. When I started this for runestaves, already with the Apophthegmata Patrum in mind, I needed target-based embeddings, that is, to train a classifier to predict categorical labels, from where we'd extract the learned representations. This gives you something like "entity embeddings": the network learns that certain rune variants cluster together not because I told it to, but because they predict the same downstream outcomes. For experiments with manuscript stemmatics, I added contrastive learning: triplet loss and siamese networks where you explicitly encode "these manuscripts should be close, those should be distant", without pretending you have supervised labels for every relationship.

## The Agent as Interpolator

Here's my working theory, not revolutionary but useful: LLMs are extraordinary interpolators in code space. When they have enough nearby examples (standard boilerplate, common patterns, well-trodden library usage, and all the necessary within contexts that fill up way too quickly) they generate clean, functional code. The manifold of "Python libraries that do X" is densely sampled in their training data. They're doing what `row2vec` does: projecting from a high-dimensional space (all possible code) onto the lower-dimensional manifold where actual working solutions live. This is mostly achieved in smaller self-correcting steps, what [Simon Willison](https://simonwillison.net/) calls "agentic" systems: LLMs calling tools in a loop to achieve a goal.

But agents fail catastrophically at the edges. Novel architectures, unusual problem formulations, domain-specific constraints, basically anywhere the local neighborhood is sparse. And, most critically, they tend to fail without knowing they're failing, even when you take care of tests and constraints. The iterated output is always smooth, always confident. You frequently get over-engineered nonsense, convoluted baroque solutions to simple problems, or worse: subtly wrong code that runs but doesn't do what you think it does.

In *The Cyberiad* of Lem (my favorite scifi author), there is a scene where Trurl, one of the protagonists, builds a machine capable of creating anything beginning with the letter N. When asked to create Nothing, it nearly destroys the universe. The joke is epistemological: the machine executes perfectly but understands nothing. This is the condition of the LLM: not unintelligent, but operating in a space where intelligence and understanding have been decoupled, interpolating brilliantly within its training manifold but unable to recognize when it has stepped beyond it. It is not a surprise that, like many others, I have also had the experience of asking to achieve 100% test coverage only to have the agent delete the failing tests instead of fixing them.

Le Guin (the second favorite) understood this distinction in *The Dispossessed*, where she describes the difference between sequential and simultaneous thinking, Cetian minds that grasp wholes rather than chains of reasoning. The LLM is the inverse: pure sequentiality, pure probability distribution over next tokens, with no grasp of the whole it's constructing. Deep down, a memory-less process. It can generate the structure of a well-formed function, the syntax of proper error handling, the boilerplate of comprehensive tests. What it cannot generate is the architectural judgment about whether this function should exist at all, whether this error can actually occur given the type system, whether these tests examine the invariants that matter.

The boring work is where the agents excel, because these are tasks where execution and understanding nearly coincide: test scaffolding, type stubs, documentation formatting, migrating from hatchling to setuptools, finding typos in comments. The manifold is dense, the patterns are standard. I've used them extensively for such bureaucratic tedium: comprehensive Makefiles, API documentation in consistent formats, test fixtures. Work that needs doing but doesn't need invention. But I still follow by hand every change, every function, every refactor. 

My approach, then, forces planning before implementation: detailed requirements analysis, alternatives with trade-offs, explicit reasoning before any code generation. I'm not teaching the agent to think better, but I need to see where it's interpolating from solid ground versus extrapolating into the void, where the training examples are dense versus where it's guessing based on distant analogy. And I've gotten good at spotting code slop from others who don't exercise such vigilance. Over-engineering is the main tell. When the agent doesn't understand the problem it compensates with structure. It is like dealing again with these programmers fresh out of Java-schools who were lucky to be at the right place and the right time. Factory patterns for three classes, elaborate configuration for five parameters, defensive checks against impossible errors. The code possesses the surface texture of thoughtfulness, the appearance of deliberation, but lacks the substrate, it lacks the architectural understanding that would make it maintainable, extendable, appropriate to the problem at hand. And now people write short prompts, accept everything, and ship without review. They trust the agent to "just work", the chat becomes a ritual without epistemology.

## The Infrastructure Argument

Here's where the parallel becomes explicit: both `row2vec` and my agent workflow are about building infrastructure for problems I can't fully specify yet, and both require a kind of knowledge that doesn't reduce to procedure.

Consider Sturt's *The Wheelwright's Shop*, published in 1923, often considered one of the finest firsthand accounts of traditional craftsmanship and a key document in the study of pre-industrial labor. It's not merely a manual on making wheels, it is from its preface a lament and celebration of a vanishing world, where skill, patience, and community defined work and identity. Sturt writes about guild knowledge that doesn't go in blueprints: the feel of wood under the tool, the accumulated judgment of which elm is suitable for hubs, how ash behaves in the steam box, the sound a spoke makes when it's properly fitted. This is knowledge that lives in the hands, in the practiced eye, in the dialogue between craftsman and material that no schematic can encode.

I am not the first to point to the echoes, after nearly a century of industrialization and mass production, in Richard Sennett's *The Craftsman* (2008). Where Sturt mourns the loss, Sennett argues for the enduring and universal relevance of craft thinking, even in our technological society. *The Craftsman* is both celebration and defense of skill, patience, and dedication in a world that often values speed and profit over quality. Craftsmanship isn't limited to woodworkers or blacksmiths, but extends to programmers, scientists, and anyone engaged in the dialogue between intention and material. This craftsman's knowledge -- tacit, embodied, emergent from practice -- is precisely what the agent cannot provide, what I must bring to the collaboration.

And this, then, is what I'm trying to practice with `row2vec`. The agent helped build this, but only the parts where the code manifold is well-explored, where competence and comprehension nearly coincide. Test boilerplate? Flawless. Build system migration? Tedious for humans, trivial for models. But the architecture -- the decision space, the design philosophy, the judgment about future problems -- that's human, that is still me, and for good or worse I can see myself in the blueprints. 

That's why I can be optimistic about the collaboration, and believe that agents will ultimately expand good workers in a landscape of slop: interpolation from the agent for well-mapped territory, architecture from the human for the edges. Supervision at every step because the failure mode is silent confidence. Again, the agent doesn't know when it's at the boundary of its training manifold. Like Trurl's machine creating Nothing, it will execute with perfect assurance tasks it doesn't understand. I have to be the one who knows, who recognizes when the code being generated has the texture of correctness without the substrate of appropriateness.

## On Releasing the Unfinished

My wave of project releases isn't about having finished anything. It's about accepting that research software is always infrastructure-in-progress, always in dialogue with problems that haven't fully materialized. The cathedral is never really done. You release when the piece is legible to future, not when it's perfect.

A certain historian wrote about artworks being abandoned rather than completed, but maybe that's too romantic. I prefer the craftsman's version: the piece is done when it can be picked up by another hand. Explicit over clever. Maintainable over shiny. Infrastructure that shows you how it works, that makes visible the decisions and trade-offs that shaped it.

The manifold will reveal itself as I go.


